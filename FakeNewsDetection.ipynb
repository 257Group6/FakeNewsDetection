{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtIRqESKdwNT"
      },
      "source": [
        "#Fake News detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8_f3Swtdqqm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1018)>\n",
            "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1018)>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import pickle\n",
        "import warnings\n",
        "import ssl\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYLB13SPlY40"
      },
      "source": [
        "###Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eJlhqPBd5BZ",
        "outputId": "a1e4fe9d-ac19-4c34-f470-3dfb02ed26ce"
      },
      "outputs": [],
      "source": [
        "df_true = pd.read_csv('data/True.csv')\n",
        "df_fake = pd.read_csv('data/Fake.csv')\n",
        "df_true.shape, df_fake.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QSzsW4cpAH1"
      },
      "source": [
        "###Drop columns \"subject\" and \"subject\". Add column \"label\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FpoWpcoFo6fI",
        "outputId": "211172e7-cb51-4cc0-fb52-eb801f9d7297"
      },
      "outputs": [],
      "source": [
        "df_true.drop(['subject', 'date'], axis=1, inplace=True)\n",
        "df_fake.drop(['subject', 'date'], axis=1, inplace=True)\n",
        "df_true['label'] = 1\n",
        "df_fake['label'] = 0\n",
        "df_true.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq7OpP3NpbMl"
      },
      "source": [
        "###Merge True and Fake dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LT8Rx1i8o-JQ",
        "outputId": "36b12e0a-2242-4d0e-bfb4-fb87d45d87f7"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df_true,df_fake]).sample(frac=1).reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k-t7z5il8yT",
        "outputId": "dce50ad3-2fe3-4b0f-c204-1e55a940dbf0"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "df['text'] = df['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = tfidf_vectorizer.fit_transform(df['text'])\n",
        "y = df['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"\\nLogistic Regression Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('model_input.dat', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "with open('vector_input.dat', 'wb') as f:\n",
        "    pickle.dump(tfidf_vectorizer, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###Deploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detectNews(text):\n",
        "    with open('model_input.dat', 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "        \n",
        "    with open('vector_input.dat', 'rb') as f:\n",
        "        vectorizer = pickle.load(f)\n",
        "\n",
        "    \"\"\"Classify news text as Real or Fake.\"\"\"\n",
        "    processed_text = preprocess_text(text)\n",
        "    text_vector = vectorizer.transform([processed_text])\n",
        "    prediction = model.predict(text_vector)\n",
        "    probability = model.predict_proba(text_vector)\n",
        "\n",
        "    label = \"Real News\" if prediction[0] == 1 else \"Fake News\"\n",
        "    confidence = probability[0][prediction[0]]\n",
        "\n",
        "    return label, confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "sample_text = \"hello world\"\n",
        "result, confidence = detectNews(sample_text)\n",
        "print(f\"Result: {result} with confidence: {confidence:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3y5F3kFqDOp"
      },
      "source": [
        "###Check for null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "Y8zBdGr8l8j6",
        "outputId": "9762d6a3-794a-4ffe-bb3f-73501ebedd1d"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtrIFpCdqacX"
      },
      "source": [
        "###Check for duplicated values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPyJJ_RSqYEg",
        "outputId": "6de5f2ce-fd77-41cb-ccd8-353294f5f32a"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "z3jHCdJFqicS"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "PJ7Oik53qxcO",
        "outputId": "b07ea137-f3ab-46f6-9d74-3f5b65f423b3"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x='label', data=df, color='g')\n",
        "plt.title(\"Title\")\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"\")\n",
        "plt.xticks([1,0], ['Real', 'Fake'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgklJdKzdzh_"
      },
      "source": [
        "#References:\n",
        "1. https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset/data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-J7ajF1d1X6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
